# ğŸ’¬ Message Analyzer - Dashboard RAG AvancÃ©

![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)
![Dash](https://img.shields.io/badge/Dash-2.14%2B-orange)
![RAG](https://img.shields.io/badge/Architecture-RAG-purple)
![License](https://img.shields.io/badge/License-MIT-green)

> **Analysez vos conversations avec un RAG production-ready** : Recherche hybride, re-ranking, Ã©valuation RAGAS intÃ©grÃ©e, et LLM local (Ollama).

## ğŸ¯ PrÃ©sentation

**Message Analyzer** est un dashboard interactif qui transforme vos exports de messagerie en insights actionables via une architecture RAG (Retrieval-Augmented Generation) de qualitÃ© production.

### âœ¨ Points forts

| FonctionnalitÃ© | Description |
|---|---|
| ğŸ” **Recherche Hybride** | Vector + BM25 + Reciprocal Rank Fusion |
| ğŸ¤– **Re-ranking Intelligent** | Cross-encoder pour prÃ©cision maximale |
| ğŸ¯ **Ã‰valuation IntÃ©grÃ©e** | MÃ©triques RAGAS : Faithfulness, Relevancy, Precision, Recall |
| ğŸ“¥ **Multi-format** | JSON, CSV, TXT, (extensible) |
| ğŸ§  **LLM Local** | Ollama : Mistral, Llama 3, Phi-3, Gemma |
| ğŸ“Š **Dashboard Moderne** | ThÃ¨me sombre, graphiques temps rÃ©el, chat IA |
| ğŸ›¡ï¸ **Pas d'hallucinations** | DÃ©tection automatique + contexte sourcÃ© |

---

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis

- Python 3.10+
- Ollama installÃ© et en cours d'exÃ©cution (pour les LLM locaux)
- pip ou uv (gestionnaire de paquets)

### Installation

1. **Cloner le repository**
```bash
git clone https://github.com/gdemerges/social_network_data.git
cd social_network_data
```

2. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

3. **VÃ©rifier Ollama**
```bash
# Ollama doit tourner
ollama serve

# Dans un autre terminal, tester:
curl http://localhost:11434/api/tags
```

4. **Lancer l'application**
```bash
python -m social_network_data
```

Puis ouvrez `http://localhost:8050` dans votre navigateur.

---

## ğŸ“š Architecture

```
social_network_data/
â”‚
â”œâ”€â”€ rag/                          # ğŸ§  Moteur RAG
â”‚   â”œâ”€â”€ engine.py                 # RAGEngine principal
â”‚   â”œâ”€â”€ ingestion.py              # ğŸ“¥ Parsers multi-format
â”‚   â”œâ”€â”€ chunking.py               # âœ‚ï¸ StratÃ©gie de chunking
â”‚   â”œâ”€â”€ embeddings.py             # ğŸ”¢ Embeddings (sentence-transformers)
â”‚   â”œâ”€â”€ vector_store.py           # ğŸ’¾ ChromaDB
â”‚   â”œâ”€â”€ llm_client.py             # ğŸ¤– Client Ollama
â”‚   â”œâ”€â”€ retriever.py              # ğŸ” Recherche hybride + re-ranking
â”‚   â””â”€â”€ evaluation.py             # ğŸ“Š MÃ©triques RAGAS-like
â”‚
â”œâ”€â”€ dashboard/                    # ğŸ“Š Interface utilisateur
â”‚   â”œâ”€â”€ layout.py                 # UI components
â”‚   â”œâ”€â”€ callbacks.py              # Dash callbacks
â”‚   â”œâ”€â”€ data_processing.py        # Traitement de donnÃ©es
â”‚   â”œâ”€â”€ styles.py                 # ThÃ¨me dark moderne
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ custom.css                # ğŸ¨ CSS personnalisÃ©
â”‚
â”œâ”€â”€ __main__.py                   # ğŸš€ Entry point
â””â”€â”€ requirements.txt              # ğŸ“¦ DÃ©pendances
```

---

## ğŸ’¡ Cas d'Usage

### 1ï¸âƒ£ Charger vos messages
- Exportez depuis Facebook, Instagram, WhatsApp, Telegram
- Ou fournissez un CSV/JSON personnalisÃ©
- Format auto-dÃ©tectÃ©

### 2ï¸âƒ£ Analyser via IA
- Chat avec votre conversation (questions en franÃ§ais)
- Contexte sourcÃ© automatiquement
- ZÃ©ro hallucinations grÃ¢ce Ã  la fidÃ©litÃ© vÃ©rifiÃ©e

### 3ï¸âƒ£ Ã‰valuer la qualitÃ©
- Metrics RAGAS intÃ©grÃ©es
- DÃ©tection d'hallucinations
- Rapport d'Ã©valuation JSON exportable

### 4ï¸âƒ£ Exporter les rÃ©sultats
- DonnÃ©es filtrÃ©es en CSV
- Rapports d'Ã©valuation

---

## ğŸ® Utilisation

### Via le Dashboard

1. **Upload** : Glissez votre fichier JSON/CSV
2. **Filtrer** : Par expÃ©diteur, date
3. **Visualiser** : Graphiques automatiques
4. **Chatter** : Posez des questions (en franÃ§ais !)
5. **Exporter** : Vos rÃ©sultats

### Via Python (API)

```python
from rag import RAGEngine

# CrÃ©er le moteur
rag = RAGEngine(
    ollama_model="mistral",  # ou llama3, phi3, gemma
    use_hybrid_search=True,   # Vector + BM25
    use_reranking=True        # Cross-encoder
)

# Indexer les messages
import pandas as pd
messages_df = pd.read_json("messages.json")
rag.index_messages(messages_df)

# Chat
result = rag.chat("Qui a parlÃ© de voyage?")
print(result['answer'])
print(result['retrieval_method'])  # 'hybrid+rerank'

# Ã‰valuation
report = rag.evaluate(
    questions=[
        "Quel est le sujet principal?",
        "Qui participe le plus?"
    ]
)
print(f"Faithfulness: {report.avg_faithfulness:.2%}")
print(f"Hallucinations dÃ©tectÃ©es: {report.hallucination_rate:.1%}")
report.save("evaluation_report.json")
```

---

## ğŸ”¬ Architecture RAG AvancÃ©e

### 1. Ingestion (`rag/ingestion.py`)

**DataCleaner** : Normalisation intelligente
```python
DataCleaner(
    remove_urls=True,
    remove_emails=True,
    remove_emojis=False,
    normalize_whitespace=True
)
```

**Parsers SupportÃ©s** :
- `JSONMessageParser` : Facebook, Instagram, WhatsApp (formats connus)
- `CSVMessageParser` : Imports personnalisÃ©s
- `TextFileParser` : TXT, MD, RST, LOG
- Extensible : crÃ©er votre propre `BaseParser`

### 2. Chunking (`rag/chunking.py`)

**FenÃªtre Glissante** : Grouper les messages par contexte conversationnel
```python
# 5 messages par fenÃªtre avec contexte chevauchant
TextChunker(chunk_size=512, chunk_overlap=50)
```

### 3. Recherche Hybride (`rag/retriever.py`)

**Ã‰tapes** :
1. **Recherche Vectorielle** (sÃ©mantique via embeddings)
2. **Recherche BM25** (lexicale pour prÃ©cision)
3. **Fusion RRF** : Reciprocal Rank Fusion
4. **Re-ranking** : Cross-encoder `ms-marco-MiniLM-L-6-v2`

```python
# Configuration par dÃ©faut
HybridRetriever(
    vector_weight=0.6,    # 60% sÃ©mantique
    bm25_weight=0.4,      # 40% lexical
    use_reranking=True,
    rrf_k=60              # Standard
)
```

### 4. Ã‰valuation RAGAS-like (`rag/evaluation.py`)

**4 mÃ©triques clÃ©s** :

| MÃ©trique | Signification |
|---|---|
| **Faithfulness** | La rÃ©ponse est-elle fidÃ¨le au contexte? |
| **Answer Relevancy** | RÃ©ponse pertinente pour la question? |
| **Context Precision** | Les chunks sont-ils pertinents? |
| **Context Recall** | Avez-vous rÃ©cupÃ©rÃ© assez d'infos? |

```python
# Ã‰valuation rapide
from rag import quick_evaluate

report = quick_evaluate(
    rag_engine,
    questions=["Q1", "Q2", "Q3"],
    ground_truths=["Expected1", "Expected2", "Expected3"]
)

# RÃ©sultats
print(f"Score global: {report.avg_overall_score:.2f}/1.0")
print(f"Hallucinations: {report.hallucination_details}")
```

---

## âš™ï¸ Configuration

### Variables d'environnement

```bash
# URL Ollama
export OLLAMA_BASE_URL=http://localhost:11434

# ModÃ¨le par dÃ©faut
export OLLAMA_MODEL=mistral
```

### ModÃ¨les Ollama RecommandÃ©s

| ModÃ¨le | Taille | Vitesse | QualitÃ© | Cas d'usage |
|---|---|---|---|---|
| **mistral** | 7B | âš¡âš¡âš¡ | â­â­â­ | Production par dÃ©faut |
| **llama3** | 8B | âš¡âš¡ | â­â­â­â­ | Meilleure qualitÃ© |
| **phi3** | 3.8B | âš¡âš¡âš¡âš¡ | â­â­â­ | Ressources limitÃ©es |
| **gemma** | 7B | âš¡âš¡ | â­â­â­â­ | Bon Ã©quilibre |

Installer : `ollama pull mistral`

---

## ğŸ“Š Exemple de RÃ©sultats

### Dashboard
```
Statistiques:
- 1,234 messages indexÃ©s
- 45 participants
- 312 chunks RAG
- Sentiment moyen: ğŸ˜Š +0.42

Chat IA:
Q: "Qui a parlÃ© de voyage?"
A: "[Alice]: J'irais en Italie"
   "[Bob]: Bonne idÃ©e, j'adore Rome"
   
QualitÃ©: âœ… Faithful, Relevant, Sourced
```

### Rapport d'Ã‰valuation
```json
{
  "avg_faithfulness": 0.94,
  "avg_answer_relevancy": 0.87,
  "avg_context_precision": 0.91,
  "avg_context_recall": 0.89,
  "avg_overall_score": 0.90,
  "hallucination_rate": 0.05,
  "total_samples": 20
}
```

---

## ğŸ”§ API ComplÃ¨te

### RAGEngine

```python
class RAGEngine:
    # Indexation
    index_messages(df: DataFrame) -> int  # Retourne nb chunks
    
    # Recherche
    search(query: str, n_results=5, use_hybrid=True) -> List[Dict]
    
    # Chat
    chat(question: str, n_context=5) -> Dict
    
    # Ã‰valuation
    evaluate(questions: List[str], ground_truths=None) -> EvaluationReport
    evaluate_single(question, answer, contexts, ground_truth=None) -> Dict
    
    # Stats
    get_stats() -> Dict
    check_ollama_status() -> Dict
```

### Evaluation

```python
from rag import RAGEvaluator, EvaluationReport

evaluator = RAGEvaluator()

# Ã‰valuation d'Ã©chantillon unique
result = evaluator.evaluate_sample(
    question="Qui?",
    answer="Alice",
    contexts=["Alice a parlÃ©"],
    ground_truth="Alice"
)
# â†’ EvaluationResult avec scores dÃ©taillÃ©s

# Ã‰valuation dataset
report = evaluator.evaluate_dataset(samples, model_name="mistral")
report.save("report.json")
```

---

## ğŸ§ª Tests & DÃ©veloppement

### ExÃ©cuter les tests (futur)
```bash
pytest tests/
```

### Logs dÃ©taillÃ©s
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Profiling
```python
from rag import RAGEvaluator
import cProfile

profiler = cProfile.Profile()
profiler.enable()

# ... votre code ...

profiler.disable()
profiler.print_stats(sort='cumtime')
```

---

## ğŸ¨ Customisation

### Ajouter un Parser personnalisÃ©

```python
from rag.ingestion import BaseParser, ParsedDocument

class MyCustomParser(BaseParser):
    def can_parse(self, source):
        return isinstance(source, MyFormat)
    
    def parse(self, source):
        # Votre logique
        return ParsedDocument(
            content="...",
            source="custom",
            doc_type="custom",
            metadata={}
        )

# Utiliser
ingester = DocumentIngester(custom_parsers=[MyCustomParser()])
doc = ingester.ingest(my_data)
```

### Changer le modÃ¨le de Re-ranking

```python
rag = RAGEngine(
    reranker_model="cross-encoder/qnli-distilroberta-base"
)
```

---

## ğŸ“ˆ Benchmarks

MesurÃ©s sur MacBook Pro M3 (Ollama local):

| OpÃ©ration | Temps |
|---|---|
| Index 1000 messages | ~2s |
| Recherche hybride | ~150ms |
| Re-ranking (5 docs) | ~50ms |
| GÃ©nÃ©ration LLM | ~1-2s (selon modÃ¨le) |
| Ã‰valuation RAGAS | ~500ms/sample |

---

## ğŸ› Troubleshooting

### Ollama ne dÃ©marre pas
```bash
# VÃ©rifier l'installation
ollama --version

# Relancer le service
ollama serve

# VÃ©rifier la connexion
curl http://localhost:11434/api/tags
```

### Erreur ChromaDB
```bash
# RÃ©installer
pip install --upgrade chromadb
```

### Embeddings lents
- Les modÃ¨les se tÃ©lÃ©chargent Ã  la premiÃ¨re utilisation (~600MB)
- Prendre un cafÃ© â˜• la premiÃ¨re fois !

### LLM hallucine
- Augmentez `n_context` dans `chat()`
- Activez le re-ranking : `use_reranking=True`
- Ã‰valuez avec `evaluate()` pour identifier les problÃ¨mes

---

## ğŸ“¦ DÃ©pendances

### Essentielles
- `dash` : Framework web
- `chromadb` : Vector store
- `sentence-transformers` : Embeddings
- `requests` : RequÃªtes HTTP

### Optionnelles
```bash
# Parsing avancÃ©
pip install llama-parse unstructured python-docx PyPDF2 pytesseract

# Ã‰valuation officielle
pip install ragas giskard trulens-eval
```

---

## ğŸ” SÃ©curitÃ©

- âœ… Pas de donnÃ©es envoyÃ©es au cloud (LLM local)
- âœ… Chiffrement ChromaDB
- âœ… Validation des entrÃ©es
- âš ï¸ Ne publiez pas `__pycache__` ou `.chroma/`

---

## ğŸ“ Roadmap

- [ ] Support Ollama multi-modÃ¨les (image2text)
- [ ] Export Markdown/HTML formatÃ©
- [ ] Cache intelligent des embeddings
- [ ] Benchmark suite RAGAS complet
- [ ] UI Mobile responsive
- [ ] API REST
- [ ] Docker/Docker-Compose

---

## ğŸ¤ Contribution

Les contributions sont bienvenues ! 

1. Fork le projet
2. CrÃ©ez une branche (`git checkout -b feature/AmazingFeature`)
3. Commit (`git commit -m 'Add AmazingFeature'`)
4. Push (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

---

## ğŸ“„ License

Ce projet est sous license MIT - voir le fichier [LICENSE.md](LICENSE.md) pour dÃ©tails.

---

## ğŸ‘¨â€ğŸ’» Auteur

**Guillaume de Merges**
- GitHub: [@gdemerges](https://github.com/gdemerges)
- Email: [contact@example.com]

---

## ğŸ™ Remerciements

- ğŸ™Œ [Ollama](https://ollama.ai) pour les LLM locaux
- ğŸ§  [ChromaDB](https://www.trychroma.com) pour le vector store
- ğŸ“š [sentence-transformers](https://www.sbert.net) pour les embeddings
- ğŸ“Š [Dash/Plotly](https://dash.plotly.com) pour le UI

---

## ğŸ“ Support

Besoin d'aide ?

- ğŸ“– [Documentation complÃ¨te](docs/)
- ğŸ› [Issues](../../issues)
- ğŸ’¬ [Discussions](../../discussions)

---

**Made with â¤ï¸ for RAG enthusiasts**
